---
title: "OzoneWatch_update_curr"
author: "Ben Wells"
date: "`r Sys.Date()`"
output: html_document
---

```{r label="get_aqs_data",echo=FALSE,message=FALSE,warning=FALSE}
aqs.userid <- Sys.getenv("AQS_userid")
aqs.passwd <- Sys.getenv("AQS_passwd")
get.aqs.data <- function(sql) {
  require(ROracle)
  driver <- dbDriver("Oracle")
  db <- "(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=db-api.aqs.epa.gov)(PORT=1521))(CONNECT_DATA=(SID=thor)))"
  con <- dbConnect(driver,username=aqs.userid,password=aqs.passwd,dbname=db)
  aqs.data <- dbGetQuery(con,sql)
  colnames(aqs.data) <- tolower(colnames(aqs.data))
  dbDisconnect(con)
  return(aqs.data)
}
```

```{r label="get_unique_dates",echo=FALSE,message=FALSE,warning=FALSE}
## Function to get earliest or latest date values by monitor
## dt = column with date or datetime values
## id = monitor id variable
## first = use earliest value (TRUE) or latest value (FALSE)?
get.unique.dates <- function(dt,id,first) {
  if (any(dt == " ")) { dt[which(dt == " ")] <- as.character(Sys.Date()) }
  if (first) { unique.dates <- tapply(dt,list(id),min) }
  if (!first) { unique.dates <- tapply(dt,list(id),max) }
  out <- sapply(unique.dates,function(x) ifelse(x == Sys.Date(),
    " ",as.character(x)))
  return(out)
}
```

```{r label="get_unique_codes",echo=FALSE,message=FALSE,warning=FALSE}
## Function to get codes associated with most recent begin dates
## dt = column with date or datetime values
## id = monitor id variable
## code = column with code values
get.unique.codes <- function(dt,id,code) {
  ids <- unique(id)
  n <- length(ids)
  out <- rep(" ",n)
  for (i in 1:n) {
    t <- which(id == ids[i])
    if (length(t) == 1) {
      out[i] <- code[t]
      next
    }
    if (any(dt[t] == " ")) {
      j <- which(dt[t] == " ")[1]
      out[i] <- code[t][j]
      next
    }
    out[i] <- code[t][which.max(as.Date(dt[t]))]
  }    
  return(out)
}
```

```{r label="get_methods",echo=FALSE,message=FALSE,warning=FALSE}
## Function to get all method codes associated with a given pollutant
get.methods <- function(par) {
  methods <- get.aqs.data(paste("SELECT DISTINCT
    methods.methodology_code AS method_code,
    COALESCE(methods.reference_method_id,' ') AS frm_fem_id,
    COALESCE(methods.sample_anal_desc,' ') AS method_desc,
    COALESCE(methods.equivalent_method_desc,' ') AS manufacturer,
    methods.fed_mdl AS mdl
    FROM sampling_methodologies methods
   WHERE methods.pa_parameter_code = '",par,"'
     AND methods.status_ind = 'P'
   ORDER BY 1",sep=""))
  methods$mdl <- as.numeric(methods$mdl)*ifelse(par == 44201,1000,1)
  if (par == 44201) { 
    ## Fix issue with AIRS legacy FRM method code
    methods$frm_fem_id[which(methods$method_code == '011')] <- "RFOA-0176-011"
    methods$manufacturer[which(methods$method_code == '011')] <- "AIRS LEGACY CODE"
  }
  return(methods)
}
```

```{r label="get_seasons",echo=FALSE,message=FALSE,warning=FALSE}
## Function to get ozone monitoring seasons for a range of years
## yr1 = get seasons with begin dates on or after this year
## yr2 = get seasons with end dates on or before this year
get.seasons <- function(yr1,yr2) {
  seasons <- get.aqs.data(paste("SELECT DISTINCT
        COALESCE(seasons.cn_stt_state_code,' ') as state,
        COALESCE(seasons.cn_county_code,' ') as county,
        COALESCE(seasons.site_id,' ') as site_id,
        COALESCE(seasons.monitoring_season_begin_month,1) as begin_month,
        COALESCE(seasons.monitoring_season_begin_day,1) as begin_day,
        COALESCE(seasons.monitoring_season_begin_year,",yr1,") as begin_year,
        COALESCE(seasons.monitoring_season_end_month,12) as end_month,
        COALESCE(seasons.monitoring_season_end_day,31) as end_day,
        COALESCE(seasons.monitoring_season_end_year,",yr2,") as end_year
   FROM monitoring_seasons seasons
  WHERE seasons.pa_parameter_code = '44201'
    AND seasons.monitoring_season_begin_year <= '",yr2,"'
    AND (seasons.monitoring_season_end_year >= '",yr1,"'
     OR seasons.monitoring_season_end_year IS NULL)
    AND seasons.cn_stt_state_code NOT IN ('80','CC')
  ORDER BY 1,2,3",sep=""))
  return(seasons)
}
```

```{r label="get_monitors",echo=FALSE,message=FALSE,warning=FALSE}
## Function to retrieve monitor metadata from AQS
## par = Get monitors with this AQS parameter code
## yr1 = Get monitors stopping after/during this year
## yr2 = Get monitors starting before/during this year
## all = Return all results (TRUE) or unique ids only (FALSE)?
get.monitors <- function(par,yr1,yr2,all=FALSE) {
  poll.table <- get.aqs.data(paste("SELECT DISTINCT
    psids.pollutant_standard_id AS psid,
    psids.pa_parameter_code AS par,
    psids.short_description AS standard
    FROM pollutant_standards psids
    WHERE psids.pa_parameter_code = '",par,"'
      AND psids.pollutant_standard_id NOT IN (3,5,6,7,18,21,24)
    ORDER BY 1,2",sep=""))
  poll.table$year <- sapply(poll.table$standard,function(x) substr(x,nchar(x)-3,nchar(x)))
  if (par == 12128) { poll.table$year <- 2009 }
  if (par == 88101) { poll.table$psid[3] <- 18 }
  t <- get.aqs.data(paste("SELECT DISTINCT
    sites.state_code || sites.county_code || sites.site_id || monitors.poc AS id,
    COALESCE(INITCAP(sites.local_site_name),' ') AS site_name,
    COALESCE(INITCAP(sites.street_address),' ') AS address,
    ROUND(COALESCE(sites.standard_latitude,sites.user_latitude),6) AS latitude,
    ROUND(COALESCE(sites.standard_longitude,sites.user_longitude),6) AS longitude,
    tz.gmt_diff AS gmt_offset,
    TO_NUMBER(regions.epa_region_code,'99') AS epa_region,
    states.state_name AS state_name,
    counties.county_name AS county_name,
    COALESCE(TRIM(cbsas.cbsa_name),' ') AS cbsa_name,
    COALESCE(TRIM(cbsas.csa_title),' ') AS csa_name,",
    if (nrow(poll.table) > 0) { 
      paste("COALESCE(
        (SELECT area_name || ', ' || state_suffix
           FROM site_naa
          WHERE sites.si_id = site_naa.si_id(+)
            AND parameter_code = '",poll.table$par[1],"'
            AND ps_id = '",poll.table$psid[1],"'),' ')
        AS naa_name_",poll.table$year[1],",",sep="")
    },
    if (nrow(poll.table) > 1) { 
      paste("COALESCE(
        (SELECT area_name || ', ' || state_suffix
           FROM site_naa
          WHERE sites.si_id = site_naa.si_id(+)
            AND parameter_code = '",poll.table$par[2],"'
            AND ps_id = '",poll.table$psid[2],"'),' ')
        AS naa_name_",poll.table$year[2],",",sep="")
    },
    if (nrow(poll.table) > 2) { 
      paste("COALESCE(
        (SELECT area_name || ', ' || state_suffix
           FROM site_naa
          WHERE sites.si_id = site_naa.si_id(+)
            AND parameter_code = '",poll.table$par[3],"'
            AND ps_id = '",poll.table$psid[3],"'),' ')
        AS naa_name_",poll.table$year[3],",",sep="")
    },
    if (nrow(poll.table) > 3) { 
      paste("COALESCE(
        (SELECT area_name || ', ' || state_suffix
           FROM site_naa
          WHERE sites.si_id = site_naa.si_id(+)
            AND parameter_code = '",poll.table$par[4],"'
            AND ps_id = '",poll.table$psid[4],"'),' ')
        AS naa_name_",poll.table$year[4],",",sep="")
    },
    "COALESCE(agencies.agency_desc,' ') AS reporting_agency,
    COALESCE(INITCAP(monitors.ms_measurement_scale),' ') AS measurement_scale,
    COALESCE(INITCAP(objectives.mot_monitor_obj_type),' ') AS monitor_objective,
    COALESCE(TO_CHAR(sp.sampling_begin_date+1/24,'YYYY-MM-DD'),' ') AS monitor_begin_date,
    COALESCE(TO_CHAR(sp.sampling_end_date+1/24,'YYYY-MM-DD'),' ') AS monitor_end_date,
    COALESCE(TO_CHAR(monitors.last_sampling_date+1/24,'YYYY-MM-DD'),' ') AS last_sample_date,
    COALESCE(TO_CHAR(exclusions.begin_date+1/24,'YYYY-MM-DD'),' ') AS nonreg_begin_date,
    COALESCE(TO_CHAR(exclusions.end_date+1/24,'YYYY-MM-DD'),' ') AS nonreg_end_date,
    COALESCE(exclusions.concurrence_flag,' ') AS nonreg_concur,
    COALESCE(methods.method_code,' ') AS method_code,
    COALESCE(sm.reference_method_id,' ') AS frm_code,
    COALESCE(TO_CHAR(methods.begin_date+1/24,'YYYY-MM-DD'),' ') AS method_begin_date,
    COALESCE(TO_CHAR(methods.end_date+1/24,'YYYY-MM-DD'),' ') AS method_end_date,
    COALESCE(mta.mt_monitor_type,' ') AS monitor_type,
    COALESCE(TO_CHAR(mta.monitor_type_begin_date+1/24,'YYYY-MM-DD'),' ') AS type_begin_date,
    COALESCE(TO_CHAR(mta.monitor_type_end_date+1/24,'YYYY-MM-DD'),' ') AS type_end_date,
    COALESCE(mna.monitor_network_code,' ') AS network,
    COALESCE(TO_CHAR(mna.begin_date+1/24,'YYYY-MM-DD'),' ') AS network_begin_date,
    COALESCE(TO_CHAR(mna.end_date+1/24,'YYYY-MM-DD'),' ') AS network_end_date,
    COALESCE(TO_CHAR(pmp.begin_date+1/24,'YYYY-MM-DD'),' ') AS primary_begin_date,
    COALESCE(TO_CHAR(pmp.end_date+1/24,'YYYY-MM-DD'),' ') AS primary_end_date,
    COALESCE(INITCAP(cf.coll_freq_desc),' ') AS collection_frequency,
    COALESCE(TO_CHAR(rcf.req_coll_freq_begin_date+1/24,'YYYY-MM-DD'),' ') AS frequency_begin_date,
    COALESCE(TO_CHAR(rcf.req_coll_freq_end_date+1/24,'YYYY-MM-DD'),' ') AS frequency_end_date,
    COALESCE(combos.second_site_state || combos.second_site_county || 
      combos.second_site_number,' ') AS combo_site,
    COALESCE(TO_CHAR(combos.cutover_date+1/24,'YYYY-MM-DD'),' ') AS combo_date,
    COALESCE(seasons.monitoring_season_begin_year,",yr1,") || '-' ||
      COALESCE(TO_CHAR(seasons.monitoring_season_begin_month,'FM09'),'01') || '-' ||
      COALESCE(TO_CHAR(seasons.monitoring_season_begin_day,'FM09'),'01') AS season_begin_date,
    COALESCE(seasons.monitoring_season_end_year,",yr2,") || '-' ||
      COALESCE(TO_CHAR(seasons.monitoring_season_end_month,'FM09'),'12') || '-' ||
      COALESCE(TO_CHAR(seasons.monitoring_season_end_day,'FM09'),'31') AS season_end_date
  FROM agencies agencies,
    cbsa_mapping cbsas,
    collection_frequencies cf,
    counties counties,
    epa_regions regions,
    linked_sites combos,
    measurement_scales scales,
    monitor_exclusions exclusions,
    monitor_methods methods,
    monitor_networks networks,
    monitor_network_affiliations mna,
    monitor_objectives objectives,
    monitor_objective_types mot,
    monitor_types types,
    monitor_type_assignments mta,
    monitoring_seasons seasons,
    monitors monitors,
    primary_monitor_periods pmp,
    req_coll_frequencies rcf,
    sample_periods sp,
    sampling_methodologies sm,
    site_basic sites,
    site_time_zone stz,
    states states,
    time_zones tz
  WHERE agencies.agency_code = monitors.monitoring_agency
    AND methods.method_code = sm.methodology_code
    AND methods.parameter_code = sm.pa_parameter_code
    AND mna.monitor_network_code = networks.monitor_network_code(+)
    AND monitors.mo_id = exclusions.mo_id(+)
    AND monitors.mo_id = methods.mo_id(+)
    AND monitors.mo_id = mna.mo_id(+)
    AND monitors.mo_id = mta.mo_mo_id(+)
    AND monitors.mo_id = objectives.mo_mo_id(+)
    AND monitors.mo_id = pmp.mo_id(+)
    AND monitors.mo_id = rcf.mo_mo_id(+)
    AND monitors.mo_id = sp.mo_mo_id(+)
    AND monitors.ms_measurement_scale = scales.measurement_scale(+)
    AND monitors.si_si_id = sites.si_id
    AND mta.mt_monitor_type = types.monitor_type(+)
    AND objectives.mot_monitor_obj_type = mot.monitor_obj_type(+)
    AND rcf.cf_coll_freq_code = cf.coll_freq_code(+)
    AND regions.epa_region_code = states.epar_epa_region_code
    AND sites.county_code = cbsas.county_code(+)
    AND sites.county_code = counties.county_code
    AND sites.si_id = combos.first_si_id(+)
    AND sites.state_code = cbsas.state_code(+)
    AND sites.state_code = counties.stt_state_code
    AND sites.state_code = states.state_code
    AND stz.si_id = sites.si_id
    AND tz.time_zone_name = stz.time_zone_name
    AND combos.parameter_code(+) = '",par,"'
    AND EXTRACT(YEAR FROM combos.cutover_date(+)) >= '",yr1,"'
    AND EXTRACT(YEAR FROM combos.cutover_date(+)) <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM exclusions.begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM exclusions.end_date(+)),",yr2,") >= '",yr1,"'
    AND COALESCE(EXTRACT(YEAR FROM methods.begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM methods.end_date(+)),",yr2,") >= '",yr1,"'
    AND COALESCE(EXTRACT(YEAR FROM mna.begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM mna.end_date(+)),",yr2,") >= '",yr1,"'
    AND COALESCE(EXTRACT(YEAR FROM monitors.last_sampling_date),",yr2,") >= '",yr1,"'
    AND monitors.last_sampling_date IS NOT NULL
    AND monitors.pa_parameter_code = '",par,"'
    AND monitors.status_ind = 'P'
    AND COALESCE(EXTRACT(YEAR FROM mta.monitor_type_begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM mta.monitor_type_end_date(+)),",yr2,") >= '",yr1,"'
    AND COALESCE(EXTRACT(YEAR FROM pmp.begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM pmp.end_date(+)),",yr2,") >= '",yr1,"'
    AND COALESCE(EXTRACT(YEAR FROM rcf.req_coll_freq_begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM rcf.req_coll_freq_end_date(+)),",yr2,") >= '",yr1,"'
    AND sites.state_code NOT IN ('80','CC')
    AND sites.status_ind = 'P'
    AND sm.pa_parameter_code = '",par,"'
    AND COALESCE(EXTRACT(YEAR FROM sp.sampling_begin_date(+)),",yr1,") <= '",yr2,"'
    AND COALESCE(EXTRACT(YEAR FROM sp.sampling_end_date(+)),",yr2,") >= '",yr1,"'
    AND sites.state_code = seasons.cn_stt_state_code(+)
    AND (sites.county_code = seasons.cn_county_code",
        ifelse(par == 44201," OR seasons.cn_county_code IS NULL)","(+))"),"
    AND (sites.site_id = seasons.site_id",
        ifelse(par == 44201," OR seasons.site_id IS NULL)","(+))"),"
    AND COALESCE(seasons.monitoring_season_begin_year(+),",yr1,") <= '",yr2,"'
    AND COALESCE(seasons.monitoring_season_end_year(+),",yr2,") >= '",yr1,"'
    AND seasons.pa_parameter_code(+) = '",par,"'
  ORDER BY 1",sep=""))
  ## Fix issues with ozone state/county/CBSA/CSA/NAA names, collection frequencies
  if (par == 44201) {
    ## Pechanga - shorten ridiculously long name
    pechanga <- grep("Pechanga",t$naa_name_2015)
    t$naa_name_2015[pechanga] <- t$naa_name_2008[pechanga] <- t$naa_name_1997[pechanga] <-
      "Pechanga Band of Luiseno Mission Indians, CA"
    ## Seaford, DE - Fix naming issue in 2008 shapefile
    seaford <- grep("Seaford",t$naa_name_2008)
    t$naa_name_2008[seaford] <- "Seaford, DE"
    ## New York Metro Area - coastline issues for 1997 NAAQS
    nyc <- which(substr(t$id,1,5) == "09001")
    t$naa_name_1997[nyc] <- "New York-N. New Jersey-Long Island Area, NY-NJ-CT"
    ## Portland, ME - coastline issues for 1997 NAAQS
    portland <- which(substr(t$id,1,5) == "23031")
    t$naa_name_1997[portland] <- "Portland Area, ME"
    ## Fixes to state/county/CBSA/CSA names for Ozone Watch
    t$state_name <- gsub("Of","of",t$state_name)
    fix <- which(substr(t$id,1,5) == "72097")
    if (length(fix) > 0) {
      t$county_name[fix] <- "Mayaguez"
      t$cbsa_name[fix] <- "Mayaguez, PR"
      t$csa_name[fix] <- "Mayaguez-San German, PR"
    }
  }
  if (all) { monitors <- t }
  if (!all) {
    monitors <- t[which(!duplicated(t$id)),]
    first <- paste(c("monitor","nonreg","method","type","network","primary","frequency"),"begin_date",sep="_")
    last <- paste(c("monitor","nonreg","method","type","network","primary","frequency"),"end_date",sep="_")
    monitors[,first] <- lapply(t[,first],get.unique.dates,id=t$id,first=TRUE)
    monitors[,last] <- lapply(t[,last],get.unique.dates,id=t$id,first=FALSE)
    monitors$last_sample_date <- get.unique.dates(t$last_sample_date,t$id,first=FALSE)
    monitors$nonreg_concur <- get.unique.codes(t$nonreg_begin_date,t$id,t$nonreg_concur)
    monitors$method_code <- get.unique.codes(t$method_begin_date,t$id,t$method_code)
    monitors$frm_code <- get.unique.codes(t$method_begin_date,t$id,t$frm_code)
    monitors$monitor_type <- get.unique.codes(t$type_begin_date,t$id,t$monitor_type)
    monitors$network <- get.unique.codes(t$network_begin_date,t$id,t$network)
    monitors$season_begin_date <- substr(get.unique.codes(t$season_begin_date,t$id,t$season_begin_date),6,10)
    monitors$season_end_date <- substr(get.unique.codes(t$season_begin_date,t$id,t$season_end_date),6,10)
  }
  return(monitors)
}
```

```{r label="setup_workspace",echo=FALSE,message=FALSE,warning=FALSE}
library(data.table); library(pins); library(plyr); library(reshape2);
options(stringsAsFactors=FALSE)
board <- board_connect()
count <- function(x) { return(sum(!is.na(x))) }
max.na <- function(x) { return(ifelse(any(!is.na(x)),max(x,na.rm=TRUE),NA)) }
mean.na <- function(x) { return(ifelse(all(is.na(x)),NA,mean(x,na.rm=TRUE))) }
date.stamp <- format(Sys.Date(),"%Y%m%d")
year <- as.numeric(substr(date.stamp,1,4))-(as.numeric(substr(date.stamp,5,6)) < 5)
d1 <- paste(year,"-01-01 00:00:00",sep="")
d2 <- paste(year+1,"-01-01 06:00:00",sep="")

## Function to calculate moving 8-hour averages
avg8 <- function(x,sub,lvl) {
  n <- count(x)
  if (n == 0) { return(NA) }
  if (n >= 6) { return(mean(x,na.rm=TRUE)) }
  if (n > 0 & n < 6) {
    x.sub <- mean(replace(x,which(is.na(x)),sub))
    return(ifelse(x.sub > lvl,x.sub,NA))
  }
}

## Function to create a time-series of the cumulative 4th highest value
max4.cum <- function(x) {
  n <- length(x)
  out <- rep(NA,n)
  for (i in 4:n) {
    t <- x[1:i]
    out[i] <- t[order(t,decreasing=TRUE)][4]
  }
  return(floor(out))
}
```

```{r label="retrieve_aqs_data",echo=FALSE,message=FALSE,warning=FALSE}
## Retrieve ozone monitor metadata and ozone monitoring seasons from AQS
monitors <- get.monitors(par=44201,yr1=year-2,yr2=year,all=FALSE)
seasons <- get.seasons(yr1=year,yr2=year)
methods <- get.methods(par=44201)

## Retrieve current year hourly ozone data from AQS
aqs <- subset(get.aqs.data(paste("SELECT DISTINCT
        rd.state_code || rd.county_code || rd.site_id || rd.poc AS id,
        TO_CHAR(rd.sampling_begin_datetime,'YYYY-MM-DD HH24:MI:SS') AS dt,
        FLOOR(GREATEST(rd.standard_sample_value*1000,0)) AS o3,
        rd.method_code AS method,
        COALESCE(rd.event_code || rd.null_data_code,' ') AS flag,
        COALESCE(rd.event_concurence_indicator || rd.null_code_concurrence,' ') AS concur
   FROM raw_data_concurrences rd
  WHERE rd.parameter_code = '44201'
    AND rd.sampling_begin_datetime >= TO_DATE('",d1,"','YYYY-MM-DD HH24:MI:SS')
    AND rd.sampling_begin_datetime <= TO_DATE('",d2,"','YYYY-MM-DD HH24:MI:SS')
    AND rd.state_code NOT IN ('80','CC')
  ORDER BY 1,2",sep="")),substr(dt,15,19) == "00:00")

## Retrieve current year hourly ozone data from AirNow
airnow <- subset(get.aqs.data(paste("SELECT DISTINCT
        sites.state_code || sites.county_code || sites.site_id || monitors.poc AS id,
        TO_CHAR(o3.sampling_begin_datetime,'YYYY-MM-DD HH24:MI:SS') AS dt,
        FLOOR(GREATEST(o3.std_sample_value*1000,0)) AS o3,
        '999' AS method,
        ' ' AS flag,
        ' ' AS concur
  FROM  airnow.raw_data o3,
        airnow.monitor_protocols mp,
        airsraqs.monitors monitors,
        airsraqs.site_basic sites
  WHERE o3.mp_mo_mo_id = mp.mo_mo_id
    AND mp.mo_mo_id = monitors.mo_id
    AND monitors.si_si_id = sites.si_id
    AND monitors.pa_parameter_code = '44201'
    AND monitors.status_ind = 'P'
    AND o3.status_ind = 'P'
    AND sites.status_ind = 'P'
    AND sites.state_code NOT IN ('80','CC')
    AND o3.sampling_begin_datetime >= TO_DATE(",d1,",'YYYY-MM-DD-HH24:MI:SS')
    AND o3.sampling_begin_datetime <= TO_DATE(",d2,",'YYYY-MM-DD-HH24:MI:SS')
  ORDER BY 1,2",sep="'")),substr(dt,15,19) == "00:00")
```

```{r label="process_hourly_data",echo=FALSE,message=FALSE,warning=FALSE}
## Combine current year AQS/AirNow hourly concentration data
temp <- rbind(aqs,airnow)
temp <- temp[!duplicated(temp[,c("id","dt")]),]
ids <- as.character(unique(temp$id))
dts <- as.character(format(seq(as.POSIXct(d1,tz="UTC"),as.POSIXct(d2,tz="UTC"),3600),format="%Y-%m-%d %H:%M:%S"))
all <- data.frame(id=rep(ids,each=length(dts)),dt=rep(dts,times=length(ids)))
aqs.airnow <- subset(merge(all,temp,by=c("id","dt"),all=TRUE),id %in% monitors$id)
aqs.airnow$date <- as.Date(substr(aqs.airnow$dt,1,10))
aqs.airnow$hour <- as.integer(substr(aqs.airnow$dt,12,13))

## Remove missing days assumed less than the standard, if any
bg <- which(aqs.airnow$flag == "BG" & aqs.airnow$concur == "Y")
if (length(bg) > 0) { aqs.airnow$o3[bg] <- -99 }

## Remove concurred exceptional events, if any
ee <- which(substr(aqs.airnow$flag,1,1) %in% c("E","R") & aqs.airnow$concur == "Y")
if (length(ee) > 0) { aqs.airnow$o3[ee] <- -99 }

## Remove data collected with non-FEM/FRM method codes, if any
non.ref <- methods$method_code[which(methods$frm_fem_id == " ")]
drop <- which(aqs.airnow$method %in% non.ref)
if (length(drop) > 0) { aqs.airnow$o3[drop] <- NA }

## Create temporary copies of original datasets
td <- aqs.airnow[,c("id","date","hour","o3")]
tm <- monitors

## Remove non-regulatory data
nr <- subset(tm,nonreg_begin_date != " " & nonreg_concur == "Y",
  select=c("id","nonreg_begin_date","nonreg_end_date"))
for (i in 1:nrow(nr)) {
  bd <- as.Date(nr$nonreg_begin_date[i])
  ed <- ifelse(nr$nonreg_end_date[i] == " ",Sys.Date(),as.Date(nr$nonreg_end_date[i]))
  if (bd > as.Date(paste(year+1,"01-01",sep="-"))) { next }
  if (ed < as.Date(paste(year,"01-01",sep="-"))) { next }
  drop <- which(td$id == nr$id[i] & td$date >= bd & td$date <= ed)
  if (length(drop) > 0) { td$o3[drop] <- NA }
}

## Combine POCs at sites with multiple monitors
sites <- substr(unique(td$id),1,9)
combine.sites <- unique(sites[which(duplicated(sites))])
td$site <- substr(td$id,1,9); tm$site <- substr(tm$id,1,9);
if (length(combine.sites) > 0) {
  sd <- vector("list",length(combine.sites))
  for (i in 1:length(combine.sites)) {
    if (!combine.sites[i] %in% tm$site) { next }
    rows <- which(td$site == combine.sites[i]); d <- td[rows,];
    m <- subset(tm,site == combine.sites[i] & primary_begin_date != " ",
      select=c("id","primary_begin_date","primary_end_date"))
    m <- m[which(!duplicated(m)),]; m <- m[order(m$primary_begin_date),];
    m$bd <- as.Date(m$primary_begin_date)
    m$ed <- sapply(m$primary_end_date,function(x) ifelse(x == " ",Sys.Date(),as.Date(x)))
    m <- subset(m,bd < as.Date(paste(year,"12-31",sep="-")) &
      ed >= as.Date(paste(year,"01-01",sep="-")))
    n.dts <- length(unique(paste(d$date,d$hour)))
    sd[[i]] <- data.frame(id="",date=d$date[1:n.dts],hour=d$hour[1:n.dts],o3=NA,
      site=combine.sites[i])
    for (j in 1:nrow(m)) {
      site.ind <- which(sd[[i]]$date >= m$bd[j] & sd[[i]]$date <= m$ed[j])
      pri.ind <- which(d$id == m$id[j] & d$date >= m$bd[j] & d$date <= m$ed[j])
      sd[[i]]$o3[site.ind] <- d$o3[pri.ind]
      site.sub <- site.ind[which(is.na(sd[[i]]$o3[site.ind]))]
      if (length(site.sub) == 0) { next }
      t <- subset(d,id != m$id[j] & date >= m$bd[j] & date <= m$ed[j])
      rep.vals <- floor(apply(matrix(t$o3,ncol=length(unique(t$id))),1,mean.na))
      sd[[i]]$o3[site.sub] <- rep.vals[(site.sub-site.ind[1]+1)]
    }
    td <- td[-c(rows),]
  }
  td <- rbind(td,as.data.frame(rbindlist(sd)))
}

## Perform site combinations
combos <- subset(tm,combo_site != " ",c("site","combo_site","combo_date"))
combos <- combos[which(!duplicated(combos)),]
if (nrow(combos) > 0) {
  combos <- combos[order(combos$combo_date),]
  for (i in 1:nrow(combos)) {
    cd <- as.Date(combos$combo_date[i])
    if (cd < as.Date(paste(year,"01-01",sep="-"))) { next }
    old.site <- combos$site[i]
    new.site <- tm$site[match(combos$combo_site[i],substr(tm$site,1,9))]
    old.rows <- which(td$site == old.site)
    new.rows <- which(td$site == new.site)
    if (length(old.rows) == 0) { next }
    if (length(new.rows) == 0) {
      td$site[old.rows] <- new.site
      next
    }
    row.copy <- old.rows[which(td$date[old.rows] < cd)]
    row.paste <- new.rows[which(td$date[new.rows] < cd)]
    if (length(row.copy) > 0) {
      td[row.paste,] <- td[row.copy,]
      td$site[row.paste] <- new.site
    }
    td <- td[-c(old.rows),]
  }
}

## Subset and sort hourly ozone data, monitor metadata
obs <- tapply(td$o3,list(td$site),function(x) sum(!is.na(x)))
td <- subset(td,site %in% names(obs[obs > 0]) & site %in% unique(tm$site))
td <- td[order(td$site,td$date,td$hour),c("site","date","hour","o3")]
tm <- subset(tm,!duplicated(site) & site %in% unique(td$site))
tm <- tm[order(tm$site),c(ncol(tm),2:(ncol(tm)-1))]
```

```{r label="calculate_dvs",echo=FALSE,message=FALSE,warning=FALSE}
## Calculate site-level design values for each ozone standard
for (a in c("U","P","I")) {
  
  ## Set constants based on current standard id
  level <- switch(a,U=70,P=75,I=84)
  naaqs <- switch(a,U="2015",P="2008",I="1997")
  req.obs <- ifelse(a == "U",13,18)
  
  ## Calculate daily and annual summary statistics for each NAAQS
  temp <- vector("list",nrow(tm))
  dates <- seq(as.Date(paste(year,"01-01",sep="-")),as.Date(paste(year,"12-31",sep="-")),1)
  n <- 24*length(dates)+7; s <- 1;
  for (m in 1:nrow(tm)) {
    ## Calculate daily max 8-hour values
    t <- td[c(s:(s+n-1)),]
    t8 <- matrix(c(t$o3[1:(n-7)],t$o3[2:(n-6)],t$o3[3:(n-5)],t$o3[4:(n-4)],
      t$o3[5:(n-3)],t$o3[6:(n-2)],t$o3[7:(n-1)],t$o3[8:n]),nrow=n-7,ncol=8)
    ma8 <- matrix(apply(t8,1,avg8,sub=ifelse(a == "U",0,2),lvl=level),nrow=24)
    obs <- apply(ma8[c(ifelse(a == "U",8,1):24),],2,count)
    if (any(t$o3 < 0,na.rm=TRUE)) { ## Remove concurred exceptional events
      t$o3[which(t$o3 < 0)] <- NA
      t8 <- matrix(c(t$o3[1:(n-7)],t$o3[2:(n-6)],t$o3[3:(n-5)],t$o3[4:(n-4)],
        t$o3[5:(n-3)],t$o3[6:(n-2)],t$o3[7:(n-1)],t$o3[8:n]),nrow=n-7,ncol=8)
      ma8 <- matrix(apply(t8,1,avg8,sub=ifelse(a == "U",0,2),lvl=level),nrow=24)
    }
    dmax <- apply(ma8[c(ifelse(a == "U",8,1):24),],2,max.na)
    temp[[m]] <- data.frame(site=rep(tm$site[m],length(dates)),date=dates,obs,dmax)
    s <- s + n
  }
  temp <- as.data.frame(rbindlist(temp))
  daily <- data.frame(site=temp$site,date=temp$date,o3=mapply(function(obs,dmax)
    ifelse(obs >= req.obs | dmax >= (level + 1),floor(dmax),NA),temp$obs,temp$dmax))
  
  ## Bring in annual design value statistics for previous 2 years
  hist.data <- pin_read(board,paste(Sys.getenv("app_owner"),"/OzoneWatch_hist",naaqs,sep=""))
  hist.vals <- as.data.frame(rbindlist(lapply(hist.data,function(x) x$dv_data)))
  site.vals <- data.frame(site=unlist(lapply(hist.data,function(x) x$site)),
                     site_name=unlist(lapply(hist.data,function(x) x$site_name)),
                       address=unlist(lapply(hist.data,function(x) x$address)),
                      latitude=unlist(lapply(hist.data,function(x) x$latitude)),
                     longitude=unlist(lapply(hist.data,function(x) x$longitude)),
                    epa_region=unlist(lapply(hist.data,function(x) x$epa_region)),
                    state_name=unlist(lapply(hist.data,function(x) x$state_name)),
                   county_name=unlist(lapply(hist.data,function(x) x$county_name)),
                     cbsa_name=unlist(lapply(hist.data,function(x) x$cbsa_name)),
                      csa_name=unlist(lapply(hist.data,function(x) x$csa_name)),
                      naa_name=unlist(lapply(hist.data,function(x) x$naa_name)))
  dv.vals <- dcast(melt(hist.vals[which(hist.vals$year %in% c((year-2),(year-1))),],
    id.vars=c("site","year")),formula=site ~ variable + year)
  vals <- merge(site.vals,dv.vals[,-c(2:7,12:15)],by="site")
  vals <- vals[which(!is.na(vals[,paste("max1",(year-2),sep="_")]) | 
    !is.na(vals[,paste("max1",(year-1),sep="_")])),]
  num.cols <- c(grep("pct",colnames(vals)),grep("exc",colnames(vals)),grep("max",colnames(vals)))
  for (i in num.cols) { vals[,i] <- as.numeric(vals[,i]) }
  
  ## Create objects used within site-level DV calculation loop
  start <- 1; types <- unlist(lapply(vals,"class"));
  all.sites <- unique(c(tm$site,vals$site))
  n.days <- ifelse(year %% 4 == 0,366,365)
  dv.list <- vector("list",length(all.sites))
  
  ## Loop over sites
  for (m in 1:length(all.sites)) {
    
    ## Get annual summary statistics for previous 2 years
    dv <- subset(vals,site == all.sites[m])
    if (nrow(dv) == 0) {
      dv <- tm[m,c(colnames(vals)[1:10],paste("naa_name",naaqs,sep="_"))]
      colnames(dv)[11] <- "naa_name"
      for (i in 12:ncol(vals)) {
        dv[,colnames(vals)[i]] <- ifelse(types[i] == "character"," ",NA)
      }
    }
    
    ## Calculate current year summary statistics
    if (m <= nrow(tm)) {
      t <- daily[start:(start + n.days - 1),]
      max.ind <- order(t$o3,decreasing=TRUE)[1:5]
      sr <- tail(intersect(which(seasons$state == substr(all.sites[m],1,2)),
        intersect(which(seasons$county %in% c(" ",substr(all.sites[m],3,5))),
        which(seasons$site_id %in% c(" ",substr(all.sites[m],6,9))))),n=1)
      ts <- seasons[sr,]
      bd <- as.Date(paste(year,ts$begin_month,ts$begin_day,sep="-"))
      ed <- as.Date(paste(year,ts$end_month,ts$end_day,sep="-"))
      req.days <- as.numeric(ed - bd + 1)
      val.days <- (t$date >= bd) & (t$date <= ed) & (!is.na(t$o3))
      pct <- sum(val.days)/req.days
      exc <- sum(t$o3 > level,na.rm=TRUE)
      start <- start + n.days
    }
    
    ## Get 5 highest daily max 8-hour values and dates
    for (i in 1:5) {
      dv[,paste("max",i,"_",year,sep="")] <- ifelse(m <= nrow(tm),t$o3[max.ind][i],NA)
      dv[,paste("date",i,"_",year,sep="")] <- ifelse(m <= nrow(tm),
        as.character(t$date[max.ind][i])," ")
    }
    
    ## Get annual percent data completeness and exceedance counts
    dv[,paste("pct",year,sep="_")] <- ifelse(m <= nrow(tm),round(100*pct),NA)
    dv[,paste("exc",year,sep="_")] <- ifelse(m <= nrow(tm),exc,NA)
    
    ## Design values for the 8-hour NAAQS (1997, 2008, and 2015)
    dv[,paste("dv",(year-2),year,sep="_")] <- floor((
      floor(dv[,paste("max4",(year-2),sep="_")]) +
      floor(dv[,paste("max4",(year-1),sep="_")]) +
      floor(dv[,paste("max4",year,sep="_")]))/3)
    dv[,paste("pct",(year-2),year,sep="_")] <- round((
      pmax(dv[,paste("pct",(year-2),sep="_")],0,na.rm=TRUE) +
      pmax(dv[,paste("pct",(year-1),sep="_")],0,na.rm=TRUE) +
      pmax(dv[,paste("pct",year,sep="_")],0,na.rm=TRUE))/3)
    dv$status <- ifelse(dv[,paste("dv",(year-2),year,sep="_")] > level,"V",
                 ifelse(dv[,paste("pct",(year-2),sep="_")] >= 75 &
                        dv[,paste("pct",(year-1),sep="_")] >= 75,"A","I"))
    if (is.na(dv$status)) { dv$status <- "I" }
    dv[,paste("critical",year,sep="_")] <- pmax(3*(level+1) - 
      floor(dv[,paste("max4",(year-2),sep="_")]) -
      floor(dv[,paste("max4",(year-1),sep="_")]),0)
    dv[,paste("critical",(year+1),sep="_")] <- pmax(3*(level+1) - 
      floor(dv[,paste("max4",(year-1),sep="_")]) -
      floor(dv[,paste("max4",year,sep="_")]),0)     
    var.list <- c("site","site_name","address","latitude","longitude","epa_region",
      "state_name","county_name","cbsa_name","csa_name","naa_name",
      paste("dv",(year-2),year,sep="_"),"status",
      paste("pct",(year-2),year,sep="_"),
      paste("pct",c((year-2):year),sep="_"),
      paste("exc",c((year-2):year),sep="_"),
      paste("critical",c(year,(year+1)),sep="_"),
      paste(paste(rep(c("max","date"),each=15),rep(1:5,times=6),sep=""),
        rep(c((year-2):year),each=5,times=2),sep="_"))
      dv.list[[m]] <- dv[,var.list]
  }
  
  ## Combine site-level DVs for current standard into a data.frame
  sitedv <- data.frame(rbindlist(dv.list))
  new.combos <- subset(combos,as.numeric(substr(combo_date,1,4)) == year)
  if (nrow(new.combos) > 0) { ## Add current year site combinations, if any
    for (i in 1:nrow(new.combos)) {
      old.site <- which(sitedv$site == new.combos$site[i])
      new.site <- which(sitedv$site == new.combos$combo_site[i])
      if (length(old.site) == 0 | length(new.site) == 0) { next }
      cols <- c(paste(rep(c("pct","exc"),each=2),rep(year-2:1,times=2),sep="_"),
        paste("critical",year,sep="_"),paste(rep(c("max","date"),each=10),
        rep(1:5,times=2),"_",rep(year-2:1,each=5),sep=""))
      sitedv[new.site,cols] <- sitedv[old.site,cols]
      sitedv[new.site,paste("pct",year-2,year,sep="_")] <- round((
        pmax(sitedv[new.site,paste("pct",year-2,sep="_")],0,na.rm=TRUE) +
        pmax(sitedv[new.site,paste("pct",year-1,sep="_")],0,na.rm=TRUE) +
        pmax(sitedv[new.site,paste("pct",year,sep="_")],0,na.rm=TRUE))/3)
      sitedv[new.site,paste("dv",year-2,year,sep="_")] <- floor((
        floor(sitedv[new.site,paste("max4",year-2,sep="_")]) +
        floor(sitedv[new.site,paste("max4",year-1,sep="_")]) +
        floor(sitedv[new.site,paste("max4",year,sep="_")]))/3)
      sitedv[new.site,"status"] <- 
        ifelse(sitedv[new.site,paste("dv",year-2,year,sep="_")] > level,"V",
        ifelse(sitedv[new.site,paste("pct",year-2,sep="_")] >= 75 &
               sitedv[new.site,paste("pct",year-1,sep="_")] >= 75,"A","I"))
      sitedv[new.site,paste("critical",year+1,sep="_")] <- pmax(3*(level+1) -
        floor(sitedv[new.site,paste("max4",year-1,sep="_")]) -
        floor(sitedv[new.site,paste("max4",year,sep="_")]),0)
      sitedv <- sitedv[-c(old.site),]
    }
  }
  sitedv <- sitedv[order(sitedv$site),]
  sitedv$naa_name <- gsub(" - Revised","",sitedv$naa_name,fixed=TRUE)
  rownames(sitedv) <- c(1:nrow(sitedv))
  assign(paste("sitedv",naaqs,sep="."),sitedv)
  
  ## Create current year site-level data objects for shiny application
  curr.data <- ddply(daily,"site",summarize,year=substr(date,1,4),day=substr(date,6,10),dmax=o3)
  site.vals <- split(curr.data,curr.data$site)
  dvs <- subset(eval(parse(text=paste("sitedv",naaqs,sep="."))),
    site %in% unique(curr.data$site) & !duplicated(site))
  dvdf <- data.frame(site=dvs$site,year=year,dv=dvs[,paste("dv",(year-2),year,sep="_")],
    status=dvs[,"status"],pct3yr=dvs[,paste("pct",(year-2),year,sep="_")],
    pct=dvs[,paste("pct",year,sep="_")],exc=dvs[,paste("exc",year,sep="_")],
    crit1=dvs[,paste("critical",year,sep="_")],crit2=dvs[,paste("critical",(year+1),sep="_")],
    max1=dvs[,paste("max1",year,sep="_")],max2=dvs[,paste("max2",year,sep="_")],
    max3=dvs[,paste("max3",year,sep="_")],max4=dvs[,paste("max4",year,sep="_")],      
    max5=dvs[,paste("max5",year,sep="_")],date1=dvs[,paste("date1",year,sep="_")],
    date2=dvs[,paste("date2",year,sep="_")],date3=dvs[,paste("date3",year,sep="_")],
    date4=dvs[,paste("date4",year,sep="_")],date5=dvs[,paste("date5",year,sep="_")])
  curr <- vector("list",length(site.vals))
  for (i in 1:length(site.vals)) {
    site.vals[[i]]$max4 <- max4.cum(site.vals[[i]]$dmax)
    curr[[i]]$site <- dvs$site[i]
    curr[[i]]$site_name <- dvs$site_name[i]
    curr[[i]]$address <- dvs$address[i]
    curr[[i]]$latitude <- dvs$latitude[i]
    curr[[i]]$longitude <- dvs$longitude[i]
    curr[[i]]$epa_region <- dvs$epa_region[i]
    curr[[i]]$state_name <- dvs$state_name[i]
    curr[[i]]$county_name <- dvs$county_name[i]
    curr[[i]]$cbsa_name <- dvs$cbsa_name[i]
    curr[[i]]$csa_name <- dvs$csa_name[i]
    curr[[i]]$naa_name <- dvs$naa_name[i]
    curr[[i]]$dv_data <- dvdf[i,]
    curr[[i]]$daily_data <- site.vals[[i]]
  }
  assign(paste("curr",naaqs,sep="."),curr)
}

## Save outputs as a Pin on the Posit Connect server
curr.list <- list(curr.1997=curr.1997,curr.2008=curr.2008,curr.2015=curr.2015,
  sitedv.1997=sitedv.1997,sitedv.2008=sitedv.2008,sitedv.2015=sitedv.2015)
pin_write(board,x=curr.list,name="OzoneWatch_curr_data",type="rds",title="OzoneWatch - current year data",
  description=paste("Last updated on",Sys.Date()),versioned=FALSE,force_identical_write=TRUE)
cat("Data updated successfully at",as.character(round(Sys.time())))
```